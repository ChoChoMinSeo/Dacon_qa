{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정 함수\n",
    "def fix_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 컨트롤\n",
    "torch.cuda.empty_cache()\n",
    "PYTORCH_CUDA_ALLOC_CONF='max_split_size_mb:128'\n",
    "# Hyperparameter\n",
    "CFG = {\n",
    "    'seed':0,\n",
    "    # CUDA 사용 가능 여부 확인\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    # 전처리용\n",
    "    'valid_size':0.05,\n",
    "    # train/vali 용\n",
    "    'batch_size':1,\n",
    "    'LR' : 2e-5, # Learning Rate\n",
    "    'EPOCHS' : 10, # 학습 Epoch\n",
    "}\n",
    "fix_seed(CFG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2', eos_token='</s>')\n",
    "# class GPTDataset(Dataset):\n",
    "#     def __init__(self,csv=pd.DataFrame):\n",
    "#         # 데이터 포맷팅 및 토크나이징\n",
    "#         formatted_data = []\n",
    "#         for _, row in tqdm(csv.iterrows()):\n",
    "#             for q_col in ['질문_1', '질문_2']:\n",
    "#                 for a_col in ['답변_1', '답변_2', '답변_3', '답변_4', '답변_5']:\n",
    "#                     # 질문과 답변 쌍을 </s> token으로 연결\n",
    "#                     input_text = row[q_col] + tokenizer.eos_token +row[a_col]\n",
    "#                     input_ids = tokenizer.encode(input_text, return_tensors='pt')[0].to(torch.int64)\n",
    "#                     formatted_data.append(input_ids)\n",
    "#         print('Done.')\n",
    "#         self.data = formatted_data\n",
    "#         tokenizer.save_pretrained(\"./hansoldeco-kogpt2\")\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "#     def __getitem__(self,idx):\n",
    "#         return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2', eos_token='</s>')\n",
    "class GPTDataset_v2(Dataset):\n",
    "    def __init__(self,csv=pd.DataFrame):\n",
    "        cont_word = [' 또한, ', ' 그리고 ']\n",
    "        categories = ['마감재','인테리어','시공','마감하자','건축구조','기타','타 마감하자']\n",
    "        # 데이터 포맷팅 및 토크나이징\n",
    "        formatted_data = []\n",
    "        for idx,(_, row) in tqdm(enumerate(csv.iterrows())):\n",
    "            for a_col in ['답변_1', '답변_2', '답변_3', '답변_4', '답변_5']:\n",
    "                # 질문과 답변 쌍을 </s> token으로 연결\n",
    "                input_text = row['질문_2'] \n",
    "                # 0.4 확률로 질문 연결\n",
    "                if random.randint(1,10)<5:\n",
    "                    next_q = csv[csv['category']==row['category']].drop(csv.index[idx]).sample(1).iloc[0,:]\n",
    "                    input_text += (random.choice(cont_word)+next_q['질문_2']+tokenizer.eos_token +row[a_col]+' '+next_q[a_col])\n",
    "                else:\n",
    "                    input_text+= (tokenizer.eos_token +row[a_col])\n",
    "                input_ids = tokenizer.encode(input_text, return_tensors='pt')[0].to(torch.int64)\n",
    "                formatted_data.append(input_ids)\n",
    "        print('Done.')\n",
    "        self.data = formatted_data\n",
    "        tokenizer.save_pretrained(\"./hansoldeco-kogpt2\")\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "644it [00:04, 156.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "data = pd.read_csv('./train.csv')\n",
    "dataset = GPTDataset_v2(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 3059, valid: 161\n"
     ]
    }
   ],
   "source": [
    "train_dataset,valid_dataset = train_test_split(dataset,test_size=CFG['valid_size'])\n",
    "print(f'train: {len(train_dataset)}, valid: {len(valid_dataset)}')\n",
    "train_loader = DataLoader(train_dataset,batch_size=CFG['batch_size'], shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset,batch_size=CFG['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(51200, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader,epoch):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(enumerate(valid_loader))\n",
    "        for batch_idx, batch in progress_bar:\n",
    "                # 데이터를 GPU단으로 이동\n",
    "                batch = batch.to(CFG['device'])\n",
    "                outputs = model(batch, labels=batch)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                # 진행률 표시줄에 평균 손실 업데이트\n",
    "                progress_bar.set_description(f\"<Validation> Epoch {epoch+1} - Avg Loss: {valid_loss / (batch_idx+1):.4f}\")\n",
    "    return valid_loss/len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,valid_loader):\n",
    "    model.to(CFG['device']) # 모델을 GPU단으로 이동\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG['LR'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-8, verbose=True)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = CFG['EPOCHS'])\n",
    "    best_score = float('inf')\n",
    "    # 모델 학습\n",
    "    for epoch in range(CFG['EPOCHS']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(enumerate(train_loader))\n",
    "        for batch_idx, batch in progress_bar:\n",
    "            # 데이터를 GPU단으로 이동\n",
    "            batch = batch.to(CFG['device'])\n",
    "            outputs = model(batch, labels=batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 진행률 표시줄에 평균 손실 업데이트\n",
    "            progress_bar.set_description(f\"<Train> Epoch {epoch+1} - Avg Loss: {total_loss / (batch_idx+1):.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = validation(model,valid_loader, epoch)\n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        # 에폭의 평균 손실을 출력\n",
    "        print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, Train Loss: {total_loss / len(train_dataset):.4f}, Valid Loss: {val_loss:.4f}\")\n",
    "        if val_loss<best_score:\n",
    "            # 모델 저장\n",
    "            print('New Minimum Valid Loss!')\n",
    "            print(\"..save current best model..\")\n",
    "            model.save_pretrained(\"./hansoldeco-kogpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader,valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    # 저장된 Fine-tuned 모델과 토크나이저 불러오기\n",
    "    model_dir = path['colab']+\"hansoldeco-kogpt2\"\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
    "    model.to(CFG['device'])\n",
    "    model.eval()\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_dir,eos_token='</s>')\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Inference를 위한 test.csv 파일 로드\n",
    "    test = pd.read_csv(path['colab']+'test.csv')\n",
    "    \n",
    "    # test.csv의 '질문'에 대한 '답변'을 저장할 리스트\n",
    "    preds = []\n",
    "\n",
    "    # '질문' 컬럼의 각 질문에 대해 답변 생성\n",
    "    gen_texts=[]\n",
    "    for test_question in tqdm(test['질문']):\n",
    "        # 입력 텍스트를 토큰화하고 모델 입력 형태로 변환\n",
    "        # input_ids = tokenizer.encode(test_question + tokenizer.eos_token, return_tensors='pt')\n",
    "        input_ids = tokenizer.encode('[Q] '+ test_question + tokenizer.eos_token+'[A] ', return_tensors='pt')\n",
    "        # 답변 생성\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids.to(CFG['device']),\n",
    "            max_length=150,\n",
    "            temperature=0.9,\n",
    "            top_k=1,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "\n",
    "        # 생성된 텍스트(답변) 저장\n",
    "        \n",
    "        for generated_sequence in output_sequences:\n",
    "            full_text = tokenizer.decode(generated_sequence, skip_special_tokens=False)\n",
    "            # 토큰화 v1\n",
    "            # answer_start = full_text.find(tokenizer.eos_token) + len(tokenizer.eos_token)+5\n",
    "            # baseline 기준\n",
    "            # answer_start = full_text.find(tokenizer.eos_token) + len(tokenizer.eos_token)\n",
    "            answer_start = full_text.find(tokenizer.eos_token) + len(tokenizer.eos_token)+5\n",
    "            answer_only = full_text[answer_start:].strip()\n",
    "            gen_texts.append([full_text[:answer_start],answer_only])\n",
    "            answer_only = answer_only.replace('\\n', ' ')\n",
    "            preds.append(answer_only)\n",
    "    pd.DataFrame(gen_texts).to_csv(path['colab']+'ref_v1+train_texts_150.csv', index=False,encoding='utf-8-sig')\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 데이터셋의 모든 질의에 대한 답변으로부터 512 차원의 Embedding Vector 추출\n",
    "# 평가를 위한 Embedding Vector 추출에 활용하는 모델은 'distiluse-base-multilingual-cased-v1' 이므로 반드시 확인해주세요.\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "# 생성한 모든 응답(답변)으로부터 Embedding Vector 추출\n",
    "pred_embeddings = model.encode(preds)\n",
    "\n",
    "# 제출용 파일 제작\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "# 제출 양식 파일(sample_submission.csv)을 활용하여 Embedding Vector로 변환한 결과를 삽입\n",
    "submit.iloc[:,1:] = pred_embeddings\n",
    "# 리더보드 제출을 위한 csv파일 생성\n",
    "submit.to_csv('./baseline_submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
