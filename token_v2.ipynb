{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2', eos_token='</s>')\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self,csv=pd.DataFrame):\n",
    "        cont_word = [' 또한, ', ' 그리고 ']\n",
    "        categories = ['마감재','인테리어','시공','마감하자','건축구조','기타','타 마감하자']\n",
    "        # 데이터 포맷팅 및 토크나이징\n",
    "        formatted_data = []\n",
    "        for idx,(_, row) in tqdm(enumerate(csv.iterrows())):\n",
    "            for a_col in ['답변_1', '답변_2', '답변_3', '답변_4', '답변_5']:\n",
    "                # 질문과 답변 쌍을 </s> token으로 연결\n",
    "                input_text = row['질문_2'] \n",
    "                # 0.4 확률로 질문 연결\n",
    "                if random.randint(1,10)<5:\n",
    "                    next_q = csv[csv['category']==row['category']].drop(csv.index[idx]).sample(1).iloc[0,:]\n",
    "                    input_text += (random.choice(cont_word)+next_q['질문_2']+tokenizer.eos_token +row[a_col]+' '+next_q[a_col])\n",
    "                else:\n",
    "                    input_text+= (tokenizer.eos_token +row[a_col])\n",
    "                input_ids = tokenizer.encode(input_text, return_tensors='pt')[0].to(torch.int64)\n",
    "                formatted_data.append(input_ids)\n",
    "        print('Done.')\n",
    "        self.data = formatted_data\n",
    "        tokenizer.save_pretrained(\"./hansoldeco-kogpt2\")\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "644it [00:02, 243.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = GPTDataset(train_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
