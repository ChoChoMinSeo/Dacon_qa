{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정 함수\n",
    "def fix_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 컨트롤\n",
    "torch.cuda.empty_cache()\n",
    "PYTORCH_CUDA_ALLOC_CONF='max_split_size_mb:128'\n",
    "# Hyperparameter\n",
    "CFG = {\n",
    "    'seed':0,\n",
    "    # CUDA 사용 가능 여부 확인\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    # 전처리용\n",
    "    'valid_size':0.05,\n",
    "    # train/vali 용\n",
    "    'batch_size':1,\n",
    "    'LR' : 2e-5, # Learning Rate\n",
    "    'EPOCHS' : 10, # 학습 Epoch\n",
    "}\n",
    "fix_seed(CFG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2')\n",
    "class Corpus_Dataset(Dataset):\n",
    "    def __init__(self,data=list()):\n",
    "        # 데이터 포맷팅 및 토크나이징\n",
    "        formatted_data = []\n",
    "        for input_text in data:\n",
    "            input_ids = tokenizer.encode(input_text, return_tensors='pt')[0]\n",
    "            if len(input_ids)>10:\n",
    "                formatted_data.append(input_ids)\n",
    "        print('Done.')\n",
    "        self.data = formatted_data\n",
    "        tokenizer.save_pretrained(\"./hansoldeco-kogpt2\")\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ref_corpus\\1 data.txt 242\n",
      "./ref_corpus\\2 data.txt 100\n",
      "./ref_corpus\\3 data.txt 338\n",
      "./ref_corpus\\4 data.txt 277\n",
      "./ref_corpus\\5 data.txt 696\n",
      "./ref_corpus\\6 data.txt 571\n",
      "./ref_corpus\\7 data.txt 98\n",
      "./ref_corpus\\8 data.txt 571\n",
      "./ref_corpus\\9 data.txt 189\n",
      "Done.\n",
      "2651\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "txts = glob.glob('./ref_corpus/*data.txt')\n",
    "data=[]\n",
    "for txt in txts:\n",
    "    with open(txt,'r',encoding='utf-8') as f:\n",
    "        sentences = f.readlines()\n",
    "        for sentence in sentences:\n",
    "            data.append(sentence)\n",
    "    print(txt,len(sentences))\n",
    "dataset = Corpus_Dataset(data)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2518, valid: 133\n"
     ]
    }
   ],
   "source": [
    "train_dataset,valid_dataset = train_test_split(dataset ,test_size=CFG['valid_size'], random_state=CFG['seed'])\n",
    "print(f'train: {len(train_dataset)}, valid: {len(valid_dataset)}')\n",
    "train_loader = DataLoader(train_dataset,batch_size=CFG['batch_size'], shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset,batch_size=CFG['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(51200, 768)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader,epoch):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(enumerate(valid_loader))\n",
    "        for batch_idx, batch in progress_bar:\n",
    "                # 데이터를 GPU단으로 이동\n",
    "                batch = batch.to(CFG['device'])\n",
    "                outputs = model(batch, labels=batch)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                # 진행률 표시줄에 평균 손실 업데이트\n",
    "                progress_bar.set_description(f\"<Validation> Epoch {epoch+1} - Avg Loss: {valid_loss / (batch_idx+1):.4f}\")\n",
    "    return valid_loss/len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,valid_loader):\n",
    "    model.to(CFG['device']) # 모델을 GPU단으로 이동\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG['LR'])\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-8, verbose=True)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = CFG['EPOCHS']*len(train_loader))\n",
    "    best_score = float('inf')\n",
    "    # 모델 학습\n",
    "    for epoch in range(CFG['EPOCHS']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(enumerate(train_loader))\n",
    "        for batch_idx, batch in progress_bar:\n",
    "            # 데이터를 GPU단으로 이동\n",
    "            # model.zero_grad()\n",
    "            batch = batch.to(CFG['device'])\n",
    "            outputs = model(batch, labels=batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 진행률 표시줄에 평균 손실 업데이트\n",
    "            progress_bar.set_description(f\"<Train> Epoch {epoch+1} - Avg Loss: {total_loss / (batch_idx+1):.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = validation(model,valid_loader, epoch)\n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        # 에폭의 평균 손실을 출력\n",
    "        print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, Train Loss: {total_loss / len(train_dataset):.4f}, Valid Loss: {val_loss:.4f}\")\n",
    "        if val_loss<best_score:\n",
    "            # 모델 저장\n",
    "            print('New Minimum Valid Loss!')\n",
    "            print(\"..save current best model..\")\n",
    "            model.save_pretrained(\"./hansoldeco-kogpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader,valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
